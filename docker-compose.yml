version: "3.9"

services:
  # ── PostgreSQL: persistent job storage ────────────────────────
  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_USER: jobscheduler
      POSTGRES_PASSWORD: jobscheduler
      POSTGRES_DB: jobscheduler
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U jobscheduler"]
      interval: 5s
      timeout: 5s
      retries: 5

  # ── Redis: inter-process job queue ────────────────────────────
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 5s
      retries: 5

  # ── FastAPI: REST API for job submission ──────────────────────
  api:
    build:
      context: .
      dockerfile: Dockerfile
    command: uvicorn api.main:app --host 0.0.0.0 --port 8000 --reload
    ports:
      - "8000:8000"
    environment:
      POSTGRES_HOST: postgres
      REDIS_HOST: redis
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - .:/app
      - ./sample_data:/data

  # ── Worker: scheduler engine + thread pool ────────────────────
  # Runs as a SEPARATE process from the API.
  # This is the correct pattern: API handles HTTP, worker handles execution.
  # You can scale workers independently: docker compose up --scale worker=3
  worker:
    build:
      context: .
      dockerfile: Dockerfile
    command: python -m worker.main
    environment:
      POSTGRES_HOST: postgres
      REDIS_HOST: redis
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - .:/app
      - ./sample_data:/data

volumes:
  pgdata:
